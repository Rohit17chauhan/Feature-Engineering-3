{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973d613e",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ccb8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale the features of a dataset\\nso that they lie within a specific range, typically between 0 and 1. This method is particularly useful in machine learning\\nwhen features have different units or magnitudes, as it ensures that each feature contributes equally to the model's training\\nprocess.\\n\\nIt is used by a sklearn liberary which are already present in python.\\nEx-from sklearn.preprocessing import MinMaxScalar\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans1=\"\"\"Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale the features of a dataset\n",
    "so that they lie within a specific range, typically between 0 and 1. This method is particularly useful in machine learning\n",
    "when features have different units or magnitudes, as it ensures that each feature contributes equally to the model's training\n",
    "process.\n",
    "\n",
    "It is used by a sklearn liberary which are already present in python.\n",
    "Ex-from sklearn.preprocessing import MinMaxScalar\n",
    "\"\"\"\n",
    "Ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37717868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data after scaling:\n",
      "       age    salary\n",
      "0  1.00000  1.000000\n",
      "1  0.37500  0.400000\n",
      "2  0.00000  0.000000\n",
      "3  0.84375  0.833333\n",
      "\n",
      "Testing data after scaling:\n",
      "     age    salary\n",
      "0  0.125  0.066667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'age': [18, 22, 30, 45, 50],\n",
    "    'salary': [20000, 22000, 32000, 45000, 50000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(train_df)\n",
    "\n",
    "# Transform the training data\n",
    "train_scaled = scaler.transform(train_df)\n",
    "\n",
    "# Transform the testing data using the same scaler\n",
    "test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# Convert the scaled data back to DataFrame for easier understanding\n",
    "train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.columns)\n",
    "test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.columns)\n",
    "\n",
    "print(\"Training data after scaling:\")\n",
    "print(train_scaled_df)\n",
    "print(\"\\nTesting data after scaling:\")\n",
    "print(test_scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80afb6",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2061c49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Unit Vector technique, also known as vector normalization or normalization to unit norm, scales the feature vector\\nof each sample to have a unit norm (typically a Euclidean norm of 1). This technique is particularly useful when the direction \\nof the feature vector matters more than its magnitude, often used in machine learning algorithms like K-means clustering and\\nsome forms of neural networks.\\nDifference-\\nMin-Max Scaling:\\n1-Adjusts values based on the minimum and maximum values of each feature across the dataset. The new values are \\nlinearly scaled.\\n2-Each feature is scaled independently, leading to each feature having the same range across all samples.\\n3-Best used when you need to normalize features to a common scale for algorithms sensitive to the magnitude of the features \\n(e.g., k-nearest neighbors, neural networks).\\n\\nExample:-\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans2=\"\"\"The Unit Vector technique, also known as vector normalization or normalization to unit norm, scales the feature vector\n",
    "of each sample to have a unit norm (typically a Euclidean norm of 1). This technique is particularly useful when the direction \n",
    "of the feature vector matters more than its magnitude, often used in machine learning algorithms like K-means clustering and\n",
    "some forms of neural networks.\n",
    "Difference-\n",
    "Min-Max Scaling:\n",
    "1-Adjusts values based on the minimum and maximum values of each feature across the dataset. The new values are \n",
    "linearly scaled.\n",
    "2-Each feature is scaled independently, leading to each feature having the same range across all samples.\n",
    "3-Best used when you need to normalize features to a common scale for algorithms sensitive to the magnitude of the features \n",
    "(e.g., k-nearest neighbors, neural networks).\n",
    "\n",
    "Example:-\n",
    "\"\"\"\n",
    "Ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d2d87d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3\n",
       "0    0.803773  0.551609  0.220644  0.031521\n",
       "1    0.828133  0.507020  0.236609  0.033801\n",
       "2    0.805333  0.548312  0.222752  0.034269\n",
       "3    0.800030  0.539151  0.260879  0.034784\n",
       "4    0.790965  0.569495  0.221470  0.031639\n",
       "..        ...       ...       ...       ...\n",
       "145  0.721557  0.323085  0.560015  0.247699\n",
       "146  0.729654  0.289545  0.579090  0.220054\n",
       "147  0.716539  0.330710  0.573231  0.220474\n",
       "148  0.674671  0.369981  0.587616  0.250281\n",
       "149  0.690259  0.350979  0.596665  0.210588\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of second question\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('iris')\n",
    "df\n",
    "from sklearn.preprocessing import normalize\n",
    "unitvec=normalize(df[['sepal_length','sepal_width','petal_length','petal_width']])\n",
    "import pandas as pd\n",
    "pd.DataFrame(unitvec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191c6b6",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b07fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a large \\nset of correlated variables into a smaller set of uncorrelated variables called principal components, while retaining most of \\nthe variance present in the original dataset. PCA achieves this by identifying the directions (principal components) along \\nwhich the variance of the data is maximized.\\n\\nPrincipal Component Analysis (PCA) is widely used in dimensionality reduction to transform a high-dimensional dataset into a \\nlower-dimensional one while preserving as much variance as possible.\\nHere is a step-by-step guide to how PCA achieves this\\n\\nStandardize the Data:\\nWhy: Features might have different scales, and standardizing ensures each feature contributes equally to the analysis.\\nHow: Subtract the mean and divide by the standard deviation for each feature.\\nXstandardized=(X‚àíŒº)/œÉ\\n\\nCompute the Covariance Matrix:\\nWhy: To understand how features vary with respect to each other.\\nHow: Calculate the covariance matrix of the standardized data\\n \\n Calculate Eigenvalues and Eigenvectors:\\n\\nWhy: Eigenvectors (principal components) provide the directions of maximum variance, and eigenvalues indicate the amount of variance in those directions.\\nHow: Solve the characteristic equation Cv=Œªv to find eigenvalues  (Œª) and eigenvectors (v).\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans3=\"\"\"Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a large \n",
    "set of correlated variables into a smaller set of uncorrelated variables called principal components, while retaining most of \n",
    "the variance present in the original dataset. PCA achieves this by identifying the directions (principal components) along \n",
    "which the variance of the data is maximized.\n",
    "\n",
    "Principal Component Analysis (PCA) is widely used in dimensionality reduction to transform a high-dimensional dataset into a \n",
    "lower-dimensional one while preserving as much variance as possible.\n",
    "Here is a step-by-step guide to how PCA achieves this\n",
    "\n",
    "Standardize the Data:\n",
    "Why: Features might have different scales, and standardizing ensures each feature contributes equally to the analysis.\n",
    "How: Subtract the mean and divide by the standard deviation for each feature.\n",
    "Xstandardized=(X‚àíŒº)/œÉ\n",
    "\n",
    "Compute the Covariance Matrix:\n",
    "Why: To understand how features vary with respect to each other.\n",
    "How: Calculate the covariance matrix of the standardized data\n",
    " \n",
    " Calculate Eigenvalues and Eigenvectors:\n",
    "\n",
    "Why: Eigenvectors (principal components) provide the directions of maximum variance, and eigenvalues indicate the amount of variance in those directions.\n",
    "How: Solve the characteristic equation Cv=Œªv to find eigenvalues  (Œª) and eigenvectors (v).\n",
    "\n",
    "\"\"\"\n",
    "Ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21324372",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6decb19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Feature extraction involves transforming the data into a set of new features that represent the original data in a more\\ninformative or compact form. Principal Component Analysis (PCA) is a powerful tool for feature extraction because it identifies\\nthe directions (principal components) along which the variance in the data is maximized. These principal components can be used\\nas new features that capture the most significant information in the data, often with reduced dimensionality.\\n\\nHow PCA is Used for Feature Extraction\\nData Standardization:\\nStandardize the features to have a mean of 0 and a standard deviation of 1. This step ensures that all features contribute\\nequally to the PCA.\\n\\nCompute the Covariance Matrix:\\nCalculate the covariance matrix to understand the relationships between the features.\\n\\nEigenvalue and Eigenvector Calculation:\\nCompute the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the directions of the new feature\\nspace (principal components), and eigenvalues indicate the amount of variance captured by each principal component.\\n\\nSelect Principal Components:\\nSort the eigenvalues in descending order and select the top \\nùëò\\nk eigenvectors (principal components). These principal components are the new features.\\n\\nTransform the Data:\\nProject the original data onto the new feature space defined by the selected principal components. This results in a \\ntransformed dataset with reduced dimensions but retained variance.\\n\\nImagine you are a data analyst working with a dataset of students' academic performance. The dataset includes several features such as:\\n\\nMath Scores\\nScience Scores\\nEnglish Scores\\nHistory Scores\\nArt Scores\\nYour goal is to simplify this dataset while retaining as much important information as possible.\\n\\nUsing PCA for Feature Extraction\\nOriginal Features:\\n\\nYou start with five features that describe each student's performance in different subjects.\\nUnderstanding Relationships:\\n\\nYou observe that some of these scores are correlated. For example, students who score high in math often score high in science,\\nand students with good English scores often perform well in history.\\n\\nPrincipal Components:\\n\\nPCA helps you identify patterns in these correlations and transforms the original features into a new set of features called principal components.\\nEach principal component is a combination of the original features and captures the most variance in the data.\\nLet's say PCA reveals that two principal components can explain 90% of the variance in the dataset.\\n\\nReduced Feature Set:\\n\\nInstead of working with all five original features, you now have two new features (principal components) that summarize the\\nimportant information from the original features.\\n\\nThese new features might represent:\\nPC1: Overall academic performance (a combination of scores in math, science, and English).\\nPC2: Creative and analytical balance (a combination of scores in art and the variability in other subjects).\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans4=\"\"\"Feature extraction involves transforming the data into a set of new features that represent the original data in a more\n",
    "informative or compact form. Principal Component Analysis (PCA) is a powerful tool for feature extraction because it identifies\n",
    "the directions (principal components) along which the variance in the data is maximized. These principal components can be used\n",
    "as new features that capture the most significant information in the data, often with reduced dimensionality.\n",
    "\n",
    "How PCA is Used for Feature Extraction\n",
    "Data Standardization:\n",
    "Standardize the features to have a mean of 0 and a standard deviation of 1. This step ensures that all features contribute\n",
    "equally to the PCA.\n",
    "\n",
    "Compute the Covariance Matrix:\n",
    "Calculate the covariance matrix to understand the relationships between the features.\n",
    "\n",
    "Eigenvalue and Eigenvector Calculation:\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the directions of the new feature\n",
    "space (principal components), and eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "Select Principal Components:\n",
    "Sort the eigenvalues in descending order and select the top \n",
    "ùëò\n",
    "k eigenvectors (principal components). These principal components are the new features.\n",
    "\n",
    "Transform the Data:\n",
    "Project the original data onto the new feature space defined by the selected principal components. This results in a \n",
    "transformed dataset with reduced dimensions but retained variance.\n",
    "\n",
    "Imagine you are a data analyst working with a dataset of students' academic performance. The dataset includes several features such as:\n",
    "\n",
    "Math Scores\n",
    "Science Scores\n",
    "English Scores\n",
    "History Scores\n",
    "Art Scores\n",
    "Your goal is to simplify this dataset while retaining as much important information as possible.\n",
    "\n",
    "Using PCA for Feature Extraction\n",
    "Original Features:\n",
    "\n",
    "You start with five features that describe each student's performance in different subjects.\n",
    "Understanding Relationships:\n",
    "\n",
    "You observe that some of these scores are correlated. For example, students who score high in math often score high in science,\n",
    "and students with good English scores often perform well in history.\n",
    "\n",
    "Principal Components:\n",
    "\n",
    "PCA helps you identify patterns in these correlations and transforms the original features into a new set of features called principal components.\n",
    "Each principal component is a combination of the original features and captures the most variance in the data.\n",
    "Let's say PCA reveals that two principal components can explain 90% of the variance in the dataset.\n",
    "\n",
    "Reduced Feature Set:\n",
    "\n",
    "Instead of working with all five original features, you now have two new features (principal components) that summarize the\n",
    "important information from the original features.\n",
    "\n",
    "These new features might represent:\n",
    "PC1: Overall academic performance (a combination of scores in math, science, and English).\n",
    "PC2: Creative and analytical balance (a combination of scores in art and the variability in other subjects).\n",
    "\"\"\"\n",
    "Ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69241bcd",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c175e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow \\nthese steps:\\n\\nUnderstand the Data: \\nReview the dataset to understand the features available and their distributions. In this case, you have features like price,\\nrating, and delivery time.\\n\\nNormalize Features: \\nSince the features may have different scales, it's essential to normalize them to bring them within the same range. Min-Max\\nscaling is a method to achieve this normalization.\\n\\nCalculate Min-Max Values:\\nFor each feature, calculate the minimum and maximum values in the dataset. This step helps in determining the range to which \\neach feature will be scaled.\\n\\nApply Min-Max Scaling Formula:\\nUse the Min-Max scaling formula to transform each feature to a common scale, typically ranging between 0 and 1.\\n\\nApply Min-Max Scaling to Each Feature:\\nIterate through each feature in the dataset and apply the Min-Max scaling formula calculated in step 4.\\n\\nUpdated Dataset: \\nReplace the original values of each feature with their scaled values obtained after applying Min-Max scaling. This will result \\nin a preprocessed dataset where all features are within the same scale range (0 to 1).\\n\\nUse Preprocessed Data for Recommendation System: \\nUtilize the preprocessed dataset with normalized features to build the recommendation system. Algorithms such as collaborative\\nfiltering or content-based filtering can then be applied to make personalized recommendations to users based on their \\npreferences and behavior.\\n\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans5=\"\"\"To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow \n",
    "these steps:\n",
    "\n",
    "Understand the Data: \n",
    "Review the dataset to understand the features available and their distributions. In this case, you have features like price,\n",
    "rating, and delivery time.\n",
    "\n",
    "Normalize Features: \n",
    "Since the features may have different scales, it's essential to normalize them to bring them within the same range. Min-Max\n",
    "scaling is a method to achieve this normalization.\n",
    "\n",
    "Calculate Min-Max Values:\n",
    "For each feature, calculate the minimum and maximum values in the dataset. This step helps in determining the range to which \n",
    "each feature will be scaled.\n",
    "\n",
    "Apply Min-Max Scaling Formula:\n",
    "Use the Min-Max scaling formula to transform each feature to a common scale, typically ranging between 0 and 1.\n",
    "\n",
    "Apply Min-Max Scaling to Each Feature:\n",
    "Iterate through each feature in the dataset and apply the Min-Max scaling formula calculated in step 4.\n",
    "\n",
    "Updated Dataset: \n",
    "Replace the original values of each feature with their scaled values obtained after applying Min-Max scaling. This will result \n",
    "in a preprocessed dataset where all features are within the same scale range (0 to 1).\n",
    "\n",
    "Use Preprocessed Data for Recommendation System: \n",
    "Utilize the preprocessed dataset with normalized features to build the recommendation system. Algorithms such as collaborative\n",
    "filtering or content-based filtering can then be applied to make personalized recommendations to users based on their \n",
    "preferences and behavior.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72891234",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ebc35ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To reduce the dimensionality of the dataset for building a model to predict stock prices using Principal Component\\nAnalysis (PCA), follow these steps:\\n\\nUnderstand the Data: Review the dataset to understand the features available. In this case, you have features such as company \\nfinancial data (e.g., revenue, profit, debt) and market trends (e.g., stock indices, interest rates, economic indicators).\\n\\nStandardize the Data: Since PCA is sensitive to the scale of the features, it's essential to standardize the data to have a\\nmean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the analysis.\\n\\nCompute the Covariance Matrix: Calculate the covariance matrix of the standardized data. The covariance matrix provides \\ninformation about the relationships between different features.\\n\\nCalculate Eigenvalues and Eigenvectors: Solve the characteristic equation of the covariance matrix to find the eigenvalues\\nand eigenvectors. Eigenvectors represent the directions (principal components) along which the data varies the most, while \\neigenvalues indicate the magnitude of variance in each direction.\\n\\nSelect Principal Components: Sort the eigenvalues in descending order and select the top \\nùëò\\nk eigenvectors corresponding to the largest eigenvalues. These principal components capture the most variance in the data and\\nwill be used as the new feature space.\\n\\nTransform the Data: Project the original standardized data onto the new feature space defined by the selected principal \\ncomponents. This results in a reduced-dimensional dataset where each sample is represented by a smaller set of principal \\ncomponents instead of the original features.\\n\\nUpdated Dataset: Replace the original features with the transformed data obtained after applying PCA. The dataset now has\\nreduced dimensionality, making it more manageable for building predictive models while retaining most of the important \\ninformation.\\n\\nModel Training: Use the reduced-dimensional dataset with principal components as input features to train your predictive model.\\nYou can use various machine learning algorithms such as regression or time series models to predict stock prices based on the \\ntransformed data\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans6=\"\"\"To reduce the dimensionality of the dataset for building a model to predict stock prices using Principal Component\n",
    "Analysis (PCA), follow these steps:\n",
    "\n",
    "Understand the Data: Review the dataset to understand the features available. In this case, you have features such as company \n",
    "financial data (e.g., revenue, profit, debt) and market trends (e.g., stock indices, interest rates, economic indicators).\n",
    "\n",
    "Standardize the Data: Since PCA is sensitive to the scale of the features, it's essential to standardize the data to have a\n",
    "mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the analysis.\n",
    "\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data. The covariance matrix provides \n",
    "information about the relationships between different features.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors: Solve the characteristic equation of the covariance matrix to find the eigenvalues\n",
    "and eigenvectors. Eigenvectors represent the directions (principal components) along which the data varies the most, while \n",
    "eigenvalues indicate the magnitude of variance in each direction.\n",
    "\n",
    "Select Principal Components: Sort the eigenvalues in descending order and select the top \n",
    "ùëò\n",
    "k eigenvectors corresponding to the largest eigenvalues. These principal components capture the most variance in the data and\n",
    "will be used as the new feature space.\n",
    "\n",
    "Transform the Data: Project the original standardized data onto the new feature space defined by the selected principal \n",
    "components. This results in a reduced-dimensional dataset where each sample is represented by a smaller set of principal \n",
    "components instead of the original features.\n",
    "\n",
    "Updated Dataset: Replace the original features with the transformed data obtained after applying PCA. The dataset now has\n",
    "reduced dimensionality, making it more manageable for building predictive models while retaining most of the important \n",
    "information.\n",
    "\n",
    "Model Training: Use the reduced-dimensional dataset with principal components as input features to train your predictive model.\n",
    "You can use various machine learning algorithms such as regression or time series models to predict stock prices based on the \n",
    "transformed data\"\"\"\n",
    "Ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db69cf",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "083f9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value=[1, 5, 10, 15, 20]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Min_Max=MinMaxScaler()\n",
    "new_value=Min_Max.fit([[1, 5, 10, 15, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93e06470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MinMaxScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">?<span>Documentation for MinMaxScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MinMaxScaler()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64904f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae8a3456",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca8245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [8.21455025e-01 1.78544975e-01 4.11274806e-34 6.05311901e-36\n",
      " 1.94264292e-68]\n",
      "Cumulative Explained Variance: [0.82145503 1.         1.         1.         1.        ]\n",
      "Number of components to retain: 2\n",
      "Original Data Shape: (5, 5)\n",
      "Transformed Data Shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'height': [150, 160, 170, 180, 190],\n",
    "    'weight': [50, 60, 70, 80, 90],\n",
    "    'age': [20, 30, 40, 50, 60],\n",
    "    'gender': ['male', 'female', 'female', 'male', 'female'],\n",
    "    'blood_pressure': [120, 130, 140, 150, 160]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical feature (gender)\n",
    "df['gender'] = df['gender'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_scaled)\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine number of components to retain (e.g., to retain 95% of variance)\n",
    "threshold = 0.95\n",
    "num_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)\n",
    "print(\"Number of components to retain:\", num_components)\n",
    "\n",
    "# Transform data using the selected number of principal components\n",
    "pca = PCA(n_components=num_components)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "print(\"Original Data Shape:\", df.shape)\n",
    "print(\"Transformed Data Shape:\", df_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df924055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
